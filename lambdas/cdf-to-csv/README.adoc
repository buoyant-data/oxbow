ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:toc: macro

= CDF to CSV

The `cdf-to-csv` is an S3 Event Notifications triggered Lambda whose
responsibility is to read a link:https://delta.io[Delta Lake] table's change
data feed in order to produce CSV output for the changed rows which can be
imported into other systems via
link:https://dev.mysql.com/doc/refman/8.4/en/load-data.html[`LOAD DATA`] or
`LOAD DATA FROM S3`

toc::[]

== Output schema

Currently `cdf-to-csv` only supports insert/update files being generated. The
lambda will rely on the prefix of the Delta table being modified. For example,
if the source table is located at `s3://datalake/schemas/myschema/mytable` and
the `CSV_OUTPUT_URL` is set to `s3://csvs/` then the prefix tree for output
`.csv` files for table versions 3, 4, 5 should look like the following:

[source]
----
s3://csvs/
├── schemas
│   ├── myshema
│   │   ├── mytable
│   │   │   ├── 3
│   │   │   │   └── inserts
│   │   │   │       ├── uuid_1.csv
│   │   │   │       ├── uuid_2.csv
│   │   │   ├── 4
│   │   │   │   └── inserts
│   │   │   │       ├── uuid_1.csv
│   │   │   │       ├── uuid_2.csv
│   │   │   ├── 5
----


== Environment Variables

|===

| Name | Default Value | Notes

| `CSV_OUTPUT_URL`
|
| Full destination URL for writing output CSV files, e.g. `s3://bucket/rootprefix`

| `DELETE_PRIMARY_KEY`
|
| Optional primary key for delete CSV files

| `CSV_DELETE_PREFIX`
|
| Prefix for output CSV files with deleted rows (requires primary key set)

| `RUST_LOG`
| `error`
| Set the log level, e.g. `info`, `warn`, `error`. Can be scoped to specific modules, i.e. `oxbow=debug`

| `UNWRAP_SNS_ENVELOPE`
| _null_
| Should only be used if S3 Event Notifications are first going to SNS and then routing to SQS for Oxbow

|===
